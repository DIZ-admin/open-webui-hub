#!/usr/bin/env python3
"""
LiteLLM Testing Script for Open WebUI Hub
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —Ñ—É–Ω–∫—Ü–∏–π LiteLLM
"""

import requests
import json
import time
import sys
from typing import Dict, List, Any

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
LITELLM_BASE_URL = "http://localhost:4000"
API_KEY = "sk-1234567890abcdef"
DASHBOARD_API_URL = "http://localhost:5002"

class LiteLLMTester:
    def __init__(self):
        self.headers = {
            "Authorization": f"Bearer {API_KEY}",
            "Content-Type": "application/json"
        }
        self.results = {
            "models_test": {},
            "generation_test": {},
            "performance_test": {},
            "errors": []
        }

    def test_models_endpoint(self) -> bool:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ endpoint –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π"""
        print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ /v1/models endpoint...")
        try:
            response = requests.get(f"{LITELLM_BASE_URL}/v1/models", headers=self.headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                models = data.get('data', [])
                self.results['models_test'] = {
                    'status': 'success',
                    'total_models': len(models),
                    'models': [model['id'] for model in models]
                }
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π")
                return True
            else:
                self.results['models_test'] = {
                    'status': 'error',
                    'error': f"HTTP {response.status_code}: {response.text}"
                }
                print(f"‚ùå –û—à–∏–±–∫–∞: {response.status_code}")
                return False
        except Exception as e:
            self.results['models_test'] = {'status': 'error', 'error': str(e)}
            print(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")
            return False

    def test_model_generation(self, model_name: str, test_prompt: str = "–ü—Ä–∏–≤–µ—Ç! –°–∫–∞–∂–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ.") -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        print(f"ü§ñ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model_name}...")
        
        payload = {
            "model": model_name,
            "messages": [{"role": "user", "content": test_prompt}],
            "max_tokens": 20
        }
        
        start_time = time.time()
        try:
            response = requests.post(
                f"{LITELLM_BASE_URL}/v1/chat/completions",
                headers=self.headers,
                json=payload,
                timeout=60
            )
            end_time = time.time()
            
            if response.status_code == 200:
                data = response.json()
                if 'choices' in data and len(data['choices']) > 0:
                    content = data['choices'][0]['message']['content']
                    usage = data.get('usage', {})
                    
                    result = {
                        'status': 'success',
                        'response_time': round(end_time - start_time, 2),
                        'content': content[:100],
                        'usage': usage,
                        'actual_model': data.get('model', model_name)
                    }
                    print(f"‚úÖ {model_name}: {result['response_time']}s - \"{content[:50]}...\"")
                    return result
                else:
                    result = {'status': 'error', 'error': 'No choices in response'}
                    print(f"‚ùå {model_name}: –ù–µ—Ç choices –≤ –æ—Ç–≤–µ—Ç–µ")
                    return result
            else:
                result = {
                    'status': 'error',
                    'error': f"HTTP {response.status_code}: {response.text[:200]}"
                }
                print(f"‚ùå {model_name}: HTTP {response.status_code}")
                return result
                
        except Exception as e:
            result = {'status': 'error', 'error': str(e)}
            print(f"‚ùå {model_name}: {e}")
            return result

    def test_all_models(self) -> None:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        if not self.test_models_endpoint():
            return
            
        models = self.results['models_test'].get('models', [])
        local_models = [m for m in models if not any(provider in m for provider in ['gpt-', 'claude-', 'gemini-'])]
        
        print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {len(local_models)} –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...")
        
        for model in local_models:
            result = self.test_model_generation(model)
            self.results['generation_test'][model] = result
            time.sleep(1)  # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

    def test_dashboard_api(self) -> None:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Dashboard API endpoints"""
        print("\nüìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Dashboard API...")
        
        # –¢–µ—Å—Ç —Å—Ç–∞—Ç—É—Å–∞
        try:
            response = requests.get(f"{DASHBOARD_API_URL}/api/litellm/status", timeout=10)
            if response.status_code == 200:
                data = response.json()
                print(f"‚úÖ Dashboard API —Å—Ç–∞—Ç—É—Å: {data.get('status', 'unknown')}")
                print(f"   –ú–æ–¥–µ–ª–µ–π: {data.get('total_models', 0)}")
            else:
                print(f"‚ùå Dashboard API —Å—Ç–∞—Ç—É—Å: HTTP {response.status_code}")
        except Exception as e:
            print(f"‚ùå Dashboard API –æ—à–∏–±–∫–∞: {e}")
        
        # –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ Dashboard API
        try:
            payload = {"model": "llama3", "message": "–¢–µ—Å—Ç —á–µ—Ä–µ–∑ Dashboard API"}
            response = requests.post(
                f"{DASHBOARD_API_URL}/api/litellm/test",
                json=payload,
                timeout=30
            )
            if response.status_code == 200:
                data = response.json()
                if data.get('status') == 'success':
                    print("‚úÖ Dashboard API –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç")
                else:
                    print(f"‚ùå Dashboard API –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {data.get('error', 'unknown')}")
            else:
                print(f"‚ùå Dashboard API –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: HTTP {response.status_code}")
        except Exception as e:
            print(f"‚ùå Dashboard API –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—à–∏–±–∫–∞: {e}")

    def performance_benchmark(self) -> None:
        """–ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        print("\n‚ö° –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
        
        test_cases = [
            {"model": "llama3", "prompt": "–ü—Ä–∏–≤–µ—Ç!", "max_tokens": 10},
            {"model": "coder", "prompt": "def hello():", "max_tokens": 20},
        ]
        
        for test_case in test_cases:
            model = test_case["model"]
            times = []
            
            print(f"   –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {model} (5 –∑–∞–ø—Ä–æ—Å–æ–≤)...")
            for i in range(5):
                start_time = time.time()
                try:
                    response = requests.post(
                        f"{LITELLM_BASE_URL}/v1/chat/completions",
                        headers=self.headers,
                        json={
                            "model": test_case["model"],
                            "messages": [{"role": "user", "content": test_case["prompt"]}],
                            "max_tokens": test_case["max_tokens"]
                        },
                        timeout=60
                    )
                    end_time = time.time()
                    
                    if response.status_code == 200:
                        times.append(end_time - start_time)
                        print(f"     –ó–∞–ø—Ä–æ—Å {i+1}: {end_time - start_time:.2f}s")
                    else:
                        print(f"     –ó–∞–ø—Ä–æ—Å {i+1}: –û—à–∏–±–∫–∞ HTTP {response.status_code}")
                        
                except Exception as e:
                    print(f"     –ó–∞–ø—Ä–æ—Å {i+1}: –û—à–∏–±–∫–∞ {e}")
                
                time.sleep(2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            
            if times:
                avg_time = sum(times) / len(times)
                min_time = min(times)
                max_time = max(times)
                
                self.results['performance_test'][model] = {
                    'avg_time': round(avg_time, 2),
                    'min_time': round(min_time, 2),
                    'max_time': round(max_time, 2),
                    'successful_requests': len(times)
                }
                
                print(f"   üìà {model}: avg={avg_time:.2f}s, min={min_time:.2f}s, max={max_time:.2f}s")

    def generate_report(self) -> None:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        print("\n" + "="*60)
        print("üìã –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø LITELLM")
        print("="*60)
        
        # –°—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–µ–π
        models_status = self.results.get('models_test', {})
        if models_status.get('status') == 'success':
            print(f"‚úÖ Endpoint /v1/models: {models_status['total_models']} –º–æ–¥–µ–ª–µ–π –¥–æ—Å—Ç—É–ø–Ω–æ")
        else:
            print(f"‚ùå Endpoint /v1/models: {models_status.get('error', 'unknown')}")
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        print(f"\nü§ñ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
        generation_results = self.results.get('generation_test', {})
        successful = sum(1 for r in generation_results.values() if r.get('status') == 'success')
        total = len(generation_results)
        print(f"   –£—Å–ø–µ—à–Ω—ã—Ö: {successful}/{total}")
        
        for model, result in generation_results.items():
            if result.get('status') == 'success':
                print(f"   ‚úÖ {model}: {result.get('response_time', 0)}s")
            else:
                print(f"   ‚ùå {model}: {result.get('error', 'unknown')[:50]}...")
        
        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        print(f"\n‚ö° –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:")
        perf_results = self.results.get('performance_test', {})
        for model, stats in perf_results.items():
            print(f"   {model}: avg={stats['avg_time']}s, —É—Å–ø–µ—à–Ω—ã—Ö={stats['successful_requests']}/5")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        with open('litellm_test_results.json', 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ litellm_test_results.json")

    def run_all_tests(self) -> None:
        """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤"""
        print("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è LiteLLM...")
        print("="*60)
        
        self.test_all_models()
        self.test_dashboard_api()
        self.performance_benchmark()
        self.generate_report()

if __name__ == "__main__":
    tester = LiteLLMTester()
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "models":
            tester.test_models_endpoint()
        elif sys.argv[1] == "generation":
            tester.test_all_models()
        elif sys.argv[1] == "performance":
            tester.performance_benchmark()
        elif sys.argv[1] == "dashboard":
            tester.test_dashboard_api()
        else:
            print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã: models, generation, performance, dashboard")
    else:
        tester.run_all_tests()
